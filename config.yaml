model:
  checkpoint: 'distilbert-base-uncased'
  num_labels: 2

training:
  learning_rate: 2e-5
  batch_size: 16
  num_epochs: 3

lora:
  r: 4
  alpha: 32
  dropout: 0.01
